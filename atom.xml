<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Research by Courtney Robinson</title>
 <link href="http://research.zcourts.com/" rel="self"/>
 <link href="http://research.zcourts.com"/>
 <updated>2013-12-29T21:42:47+00:00</updated>
 <id>http://research.zcourts.com</id>
 <author>
   <name>Courtney Robinson</name>
   <email>research@crlog.info</email>
 </author>

 

 
 <entry>
   <title>Not found</title>
   <link href="http://research.zcourts.com/404.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">

Sorry this page does not exist :(

Check out the other crap I've been doing for my research though...I'm sure there's something to peek your interest.

## Categories

&lt;ul&gt;
  
  


  
    
  


&lt;/ul&gt;

## Posts
&lt;div&gt;




  


&lt;/div&gt;

## Tags

&lt;ul&gt;
  
  


  
    
  



&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Archive</title>
   <link href="http://research.zcourts.com/archive.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">





  

</content>
 </entry>
 
 <entry>
   <title>Atom Feed</title>
   <link href="http://research.zcourts.com/atom.xml"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;feed xmlns=&quot;http://www.w3.org/2005/Atom&quot;&gt;
 
 &lt;title&gt;{{ site.title }}&lt;/title&gt;
 &lt;link href=&quot;{{ site.production_url }}/{{ site.atom_path }}&quot; rel=&quot;self&quot;/&gt;
 &lt;link href=&quot;{{ site.production_url }}&quot;/&gt;
 &lt;updated&gt;{{ site.time | date_to_xmlschema }}&lt;/updated&gt;
 &lt;id&gt;{{ site.production_url }}&lt;/id&gt;
 &lt;author&gt;
   &lt;name&gt;{{ site.author.name }}&lt;/name&gt;
   &lt;email&gt;{{ site.author.email }}&lt;/email&gt;
 &lt;/author&gt;

 {% for post in site.posts %}
 &lt;entry&gt;
   &lt;title&gt;{{ post.title }}&lt;/title&gt;
   &lt;link href=&quot;{{ site.production_url }}{{ post.url }}&quot;/&gt;
   &lt;updated&gt;{{ post.date | date_to_xmlschema }}&lt;/updated&gt;
   &lt;id&gt;{{ site.production_url }}{{ post.id }}&lt;/id&gt;
   &lt;content type=&quot;html&quot;&gt;{{ post.content | xml_escape }}&lt;/content&gt;
 &lt;/entry&gt;
 {% endfor %}

 {% for page in site.pages %}
 &lt;entry&gt;
   &lt;title&gt;{{ page.title }}&lt;/title&gt;
   &lt;link href=&quot;{{ site.production_url }}{{ page.url }}&quot;/&gt;
   &lt;updated&gt;{{ page.date }}&lt;/updated&gt;
   &lt;id&gt;{{ site.production_url }}{{ page.id }}&lt;/id&gt;
   &lt;content type=&quot;html&quot;&gt;{{ page.content | xml_escape }}&lt;/content&gt;
 &lt;/entry&gt;
 {% endfor %}
 
&lt;/feed&gt;</content>
 </entry>
 
 <entry>
   <title>Bibliography</title>
   <link href="http://research.zcourts.com/bibliography.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}

{% bibliography %}</content>
 </entry>
 
 <entry>
   <title>Categories</title>
   <link href="http://research.zcourts.com/categories.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}

&lt;ul class=&quot;tag_box inline&quot;&gt;
  {% assign categories_list = site.categories %}
  {% include JB/categories_list %}
&lt;/ul&gt;


{% for category in site.categories %} 
  &lt;h2 id=&quot;{{ category[0] }}-ref&quot;&gt;{{ category[0] | join: &quot;/&quot; }}&lt;/h2&gt;
  &lt;ul&gt;
    {% assign pages_list = category[1] %}  
    {% include JB/pages_list %}
  &lt;/ul&gt;
{% endfor %}

</content>
 </entry>
 
 <entry>
   <title>High throughput file system with guaranteed sequential IO</title>
   <link href="http://research.zcourts.com/file-system-and-io.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}

# Sequential read and write file system

Read and write access on spindle disks will perform better if the data to be read and written can be done sequentially. The read throughput is far more important for query performance. To address this a novel technique which builds on earlier work (LSM) is proposed.

## Write journal / Write ahead log

A journal is a simple log of all actions to be performed on data. The contents of a journal can be used to recover the entire file system be replaying each action sequentially.
* All write requests are written to the journal.
* All requests are final, once written it can be read but not edited

## Sequentially consistent commit file

As a graph evolves (more vertices or edges added) the structure can become suboptimal for IO throughput as the disk head is having to do more repositioning to access the next vertex in a traversal.
A graph database has the advantage of knowing exactly what data it will access next. Taking advantage of this fact can improve the read throughput by reducing the number of repositioning the disk head has to do.

* Every vertex knows it's edges and hence its adjacent vertices.
* All properties of a vertex can be stored next to each other. Properties here means all a vertex's data attributes and all the edge information (and their data attributes) associated with the vertex.
* After a vertex's properties are written, the next vertex and it's properties are written in the same way, where the next vertex is one of the adjacent vertices.
* Determining which vertex is written next can be determined by whether the file system is optimized for depth or breadth first traversal. This is also a micro-optimization since the very act of placing one of the next vertices after the current one already minimizes how much the disk head has to seek. For a vertex with a small number of adjcent vertices or a large number of small vertices the  movement of the disk head is likely small enough to be negligable.

# Online file compaction

To avoid locking immutability is used where reasonable to do so. This means that the entire filesystem is largely write only. A side effect of this is that the commit file does not remain sequentially consistent as data can only be appended. In order to remain consistently sequential, some sort of re-ordering must be done. To achieve this, the slinky compaction mechanism is proposed.

## Slinky Compaction for append only file systems

Taking ideas from Haskell and Cassandra. Using an append only data structure requires compaction in order to clean up deleted or out of date versions.

Instead of doing a background clean up task which can hog resources. Use two files one is the current file being appended to and the second is the previously written to file. Data from the old file pours over from the old file to the new one while it also accepts new data. The fact that old and new data is interleaved doesn't matter because the file index is also written with this slinky like format. Once old data pours over completely into the new one, the old file is removed and the new one continues to be used until it is determined that the process should start again. This reduces the overhead associated with compactions. I've done the experiment with files up to 150GB and recorded a large improvement. Details and exact numbers will be published when I get the time to write everything up in Jan/Feb 2014. (TODO: Write up a page with the results and prepare a paper for publishing)

# Generational data indexing

The journal and slinky mechanism introduces 3 potential locations for data to be available from. In order to avoid random disk seeks and data searches over potentially several gigabytes or terabytes of data, the file system uses a vertex index, where, given any vertex ID, the index can give the exact byte offset at which the vertex's data begines and in the correct file.

## Garbage collection style indexing

Trishul M. Chilimbi and James R. Larus (Using Generational Garbage Collection To Implement  Cache-Conscious Data Placement) explored how a generational garbage collector can be used to re-organize data structures in object oriented languages, such that objects with a high temporal affinity are placed next to each other to increate the likely hood of those objects being placed in the same cache block.

While their goals are somewhat different a similar technique can be used for disk based file systems.
There are three primary files involved in the file system and at any point the system must be able to specifically identify which file and which byte offset a vertex's data starts at.

Classify each file as a generation such that eden or first generation is the journal, tenured or second generation is old commit file to be compacted and perm or third generation is the new commit file being written to.

When an operation is performed that modifies data e.g. add, remove, &quot;update&quot;, vertex or edge, this is written to the journal. 

A record is then placed in the index acknowledging the existence of the data, if something was added, if it was removed the record is removed from the index.

### Add vertex or edge

When a vertex, edge or an attribute of either is added, the operation is written to the journal. After it is written to the journal the index is updated with the byte offset. The data remains in this generation until sweep operation is performed.


### Edit vertex or edge
Edit's are strictly speaking not supported as a user operation. Any action akin to an edit actually creates a new version of the old data. No in place edit is exposed to the user.

## Remove vertex or edge

main point is write to journal then gc phase copy to commit log after commit log update index, after index points to new location log can optionally be truncated.
point is that index isn't update until after data exists in the next generation.

then like GC 2nd gen is copied to 3rd gen but again index isn't updated until after data is commited to 3rd gen


during slinky compaction deleted data is not copied from the 2nd to the 3rd gen hence resulting in compaction

# Data format specification
The byte structure of the data is needed to ensure consistency across implementations.

## File segmentation

The amount of data in a single file can grow to the point where seeking becomes a bottle neck. To avoid this, all files (persistent index and 1st to 3rd gen data files) are segmented when the data reaches a given size, m. What this means for the in memory data structures is that they need to be segment aware. Segments are simply named incrementally. 

To avoid approaching or hitting the OS's file system limits the segmentation (e.g. 31998 sub-directory limit in ext3, 16GB max file size with 1KB blocks) is taken further.

## Index format

</content>
 </entry>
 
 <entry>
   <title>Distributed Graph Database</title>
   <link href="http://research.zcourts.com/index.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}


# What?

What am I researching? Simply put, I'm trying to find a practical, scalable way to build a distributed graph database.

### Why, isn't there a lot of work being done on that front already?

Yes. In fact, the last 30 or so years has seen a significant amount of work being done in the graph field. However, we have yet to realize the full potential of these, almost ubiquitous structures. Graph traversals in particular remain an extremely difficult thing to do on very large data sets.

### Another Big Data project?

__No this isn't another &quot;Big Data&quot; blah blah.__ Recent strides in data warehousing/processing have had both a good and bad impact on the field. I'm of the opinion that it is actually unfortunate that Graph databases have been lobbed under the whole Big Data marketing hype. At the same time, the exposure has increased the amount of research in the area so I'm somewhat in two minds about that.

# Why

I'm a big fan of [Cassandra](http://cassandra.apache.org) {% cite cassandra %}. I've been working with it since 2008, not long after it was open sourced by Facebook.

My fascination with Graphs started when I realized that a lot of the work I did, a lot of what I know other people use Cassandra for could actually be represented and queried more efficiently as Graphs.

I explored existing projects like [Titan](http://thinkaurelius.github.io/titan/) and went through related projects in the [TinkerPop Stack](http://www.tinkerpop.com/).
Twitter's [FlockDB](https://github.com/twitter/flockdb), [Mizan](http://thegraphsblog.wordpress.com/the-graph-blog/mizan/) {% cite mizan %} and others, but they all fall short in one way or another. Having said that, GraphLab have an excellent technique known as [GraphChi](http://graphlab.org/graphchi/) or rather the algorithm is called &quot;Parallel sliding windows&quot; {% cite graphchi %} - I'm combining this with my own distributed partition algorithm (paper soon).

# It's all about speed

Ultimately the big issue with distributed graph traversal comes down to speed. And that's all there is to it...sort of.

Actually the big problem is partitioning a dynamic, evolving distributed graph. The location of a vertex affects how fast it and it's edges can be traversed in a query (so speed).

# Premise, Current line of thinking

The cost of accessing vertices across servers (nodes) depends mainly on the speed of the network. If local operations can be optimized to be (potentially) orders of magnitude faster than network operations and local ops. are performed much more frequently than network ops. then the __amortized__ cost of a distributed traversal should be more than acceptable for most scenarios.  Ideally traversal over data up to 1TB should be no more than a few seconds, but to be hopeful a sub-second target is on hand.

# Amortization... you what now?

[Robert Tarjan's, Amortized Computational Complexity]( /papers/Amortized%20Computational%20Complexity.pdf ) {% cite amortization %} formally introduced the idea of amortized computational costs. It's a simple but note worthy idea. My premise is in effect rested on this idea. That is, computational complexity isn't a simple matter of determining 'big O'. A system's purpose is to perform a given set of tasks, these tasks typically are broken down into smaller tasks. Each requiring a varying amount of computing resources/time. Amortization rests on the idea that some tasks are more &quot;expensive&quot; than others. If there are enough &quot;cheap&quot; tasks, each time they run the acrue &quot;credit&quot; that can eventually be spent performing the more expensive tasks. It follows that, if there are enough cheap tasks to consistently provide enough credit then the effects of performing more expensive tasks can effectively be thought of has being negligable...or there abouts.

Naturally I paraphrased that. And that is, at least in my mind a decent interpretation of amortized costs/complexity.

# Who?

Okay, okay, I got the idea. You're trying to do something that's been done 50 times already. Who are you anyway?

Well, my name is Courtney Robinson. On the web you can find me under the username &quot;zcourts&quot;. If it matters, If I'm a member, I'll be registered under that username, unless the service doesn't allow me to...in which case that service probaly doesn't matter so I'm not registered :P.
I have a 1st class BEng, Hons Software Engineering degree from Greenwich University. I'm a proud born and largely raised Jamaican, naturalized Briton and your not so every day hacker. I'm in my early 20s building awesome stuff at [DataSift](http://datasift.com).

I started working on this project during my final year while I was attempting to build my own startup.

# What's it called?

It's a bit early to be naming &quot;it&quot;, even though I've been building this for about a year, I'm not ready to release it yet. However, I have named it, I really wanted to call it &quot;Tesseract&quot;, so much so that when all permutations of that domain name were unavailable, I tried all permutations of it's translation that still sounded cool, and that I could actually pronounce. Eventually came accross hiperkubo which is Esperanto for Tesseract. I didn't like the entire thing for a name but quite liked kubo. So I registered &quot;[kubo.io](http://kubo.io)&quot;.

# What's it written in?

__Haskell__ (some C and C++ here and there)! At first my only reason for choosing Haskell was because I wanted to learn it. As it turned out, many ideas from the functional programming world are excellent for a database and I'm trying to take advantage of all of them. Saying that, I've also seen the problems Cassandra had in the early days where it was entirely reliant on the JVM for memory management. I wanted a memory managed environment away from the JVM that ideally compiled to C/assembly. [Chris Okasaki's, Purely Functional Data Structures]( /papers/okasaki.pdf ) {% cite okasaki %} is one key bit of reading that has helped influnce some of my design choices and solidified Haskell as the right language for the job.

# Is it open source?

It will be! I'm planning to release it under either a 3-clause BSD or the Apache v2 license. 

So why haven't I done that yet? A lot of what I've created in the last year (just over a year now I think) or so are pieces of the puzzle. Independent experiments that together will form a complete system, but the glue to get them together as one hasn't been written yet so it's not fully functional as a system. I have experiments in three areas, file systems, query engine and data caching. Effectly all the pieces are there but I won't bring them together until I'm satisfied with the performance/algorithms of each independently.

# Pages

&lt;ul&gt;
  {% assign pages_list = site.pages %}
  {% include JB/pages_list %}
&lt;/ul&gt;

## Categories

&lt;ul&gt;
  {% assign categories_list = site.categories %}
  {% include JB/categories_list %}
&lt;/ul&gt;

## Posts
&lt;div&gt;
{% assign posts_collate = site.tags.homepage %}
{% include JB/posts_collate %}
&lt;/div&gt;

## Tags

&lt;ul&gt;
  {% assign tags_list = site.tags %}
  {% include JB/tags_list %}
&lt;/ul&gt;

## References

{% bibliography -c %}


Built with &lt;a href=&quot;http://jekyllbootstrap.com&quot; target=&quot;_blank&quot;&gt;Jekyll Bootstrap&lt;/a&gt; and &lt;a href=&quot;http://github.com/dhulihan/hooligan&quot; target=&quot;_blank&quot;&gt;The Hooligan Theme&lt;/a&gt;</content>
 </entry>
 
 <entry>
   <title>JSC Demo page</title>
   <link href="http://research.zcourts.com/jsc.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}

Mark down using cite {% cite pagh2005optimal %}, is new but invention x {% cite boom2 %} is awesome. Twas a creation of J's {% cite boom3 %} {% cite kirsch2006distance %}. Do you know what {% m %}e = mc^2{% em %} actually means? But do you really?

What about this ?

{% math %}
\mathbf{V}_1 \times \mathbf{V}_2 =  \begin{vmatrix}
\mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \\
\frac{\partial X}{\partial u} &amp;  \frac{\partial Y}{\partial u} &amp; 0 \\
\frac{\partial X}{\partial v} &amp;  \frac{\partial Y}{\partial v} &amp; 0
\end{vmatrix} 
{% endmath %}

## References
{% bibliography -c %}</content>
 </entry>
 
 <entry>
   <title>Mega Push Gossip - MPG</title>
   <link href="http://research.zcourts.com/mega-push-gossip-mpg.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}

# Mega Push Gossip - MPG

Mega (as in Megaphone) Push Gossipe, MPG is a gossip algorithm favouring push epidemic but taking advantage of pull where appropriate.

There are lots of gossip algorithms out there. Most of them use a communication model that doesn't directly send messages to nodes in the cluster, instead, information is disseminated by spreading the message with a sub-set of the nodes that'll in turn spread the message to another sub-set until all nodes have gotten the message. Often it takes O(log(n)) hops for all nodes to be updated.

While this is perfectly reasonable and appropriate for the scenarios they were designed for, it is somewhat unnecessary in a distributed database scene. Typically a single cluster has machines in the thousands, not hundredsof thousands or millions, as evidenced by the usage of projects like Hadoop, where [Yahoo!](http://wiki.apache.org/hadoop/PoweredBy#Y) with the largest known cluster is only at 4500 nodes.

Current distributed databases are not really distributed. While their architecture tends to be designed to scale to millions or billions of nodes in a cluster, other practical limitations (usually network/hardware related) often prevent them getting anywhere near the theoretical limits of the architecture/algorithms.

#### Differences and similarities to other gossip algorithms

* MPG is a modification and merger of various gossip protocols. In reality Push Push Pull is a somewhat more suitable name because it ends up pushing at least twice as much information as it pulls. 

* The modification in the protocol is the use of O(1) communication for most operations, or all if possible. 

* Where latency or resource constraints demand it, a fanout approach is used to disamminate messages in O(log(n)) hops.

* In a typical gossip protocol a single node has a partial view of the cluster, MPG changes this so that every node has a complete view, with direct access to routing information of each.

### True cost of gossip

Gossip algorithms are typically described in terms of the number of network hops required to get all nodes up to date. While some mention the total time it takes the focus is often on network hops. With MPG, the cost of gossip has to be calculated differently. MPG assumes TCP over UDP for transmission to help improve delivery reliability. 
 
#### Other Gossip protocols

Firstly, let's define some terms that may be used to discuss the properties of one gossip protocol.
Let:

1. `b` be buffer size of a node, i.e. the number of messages it buffers
2. `t` be a limited number of hops or time steps, i.e. how far down the hierarchy a message from a node is disseminated. If it starts at A in a cluster up to F, where does dissemination stop, B,C,D,..?
3. `f` be the number of randomly select nodes a message is forwarded to each time, i.e. how many nodes it sends messages to
4. `n` be the number of nodes in the cluster.
5. __`R`__ be the number of _r_ounds/hops required for all nodes to be updated

![Diagram of message dissemination from UCL](/assets/gossip-b-f-t.png &quot;Diagram of message dissemination from UCL&quot;)



In other gossip protocols if a cluster has 100 nodes and communication of a message is required. Assuming each node

#### Sources

* [Université Catholique de louvain, UCL - Gossip lecture](http://www.info.ucl.ac.be/courses/SINF2345/2010-2011/slides/10-Gossip-lecture-hand.pdf)
* [Gossip Algorithms, MIT](http://web.mit.edu/vdb/www/6.977/l-shah.pdf)
* [T-Man: Fast gossip-based construction of large-scale overlay topologies](http://lex104.cs.unibo.it/pub/UBLCS/2004/2004-07.pdf)

## References
{% bibliography -c %}</content>
 </entry>
 
 <entry>
   <title>Pages</title>
   <link href="http://research.zcourts.com/pages.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}

&lt;h2&gt;All Pages&lt;/h2&gt;
&lt;ul&gt;
{% assign pages_list = site.pages %}
{% include JB/pages_list %}
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Passing thoughts</title>
   <link href="http://research.zcourts.com/passing-thoughts.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}

## Possible approach to a compressed edge representation

Vertex is sent to each node and assumes a fair if not even distribution of vertices accross nodes.

Each node builds a bit graph of the vertices it currently owns.
A bit graph is a structure that represents edges using a single bit for each vertex.
Each item in the bit set is made of 2 bits. first bit is a vertex and the second bit is the vertex it connects to. if the 3rd bit is a 0 it means the 4th bit represents a vertex not connected to the previous bit/vertex....

locally each node stores a mapping of the bit position to the local vertex id

the bit graph is sent to the tesseri node from all nodes when their vertex set changes over a given threshold. the tesseri then has a complete view of the cluster and can do clustering over the entire graph because each edge is now represented by 2 bits.

This means we can use 8GB to represent up to 34359738368 edges (34.35 billion edges) big enough for many scenarios. Trouble with this and any approach which hoard the entire cluster on one machine is it doesn't take advantage of multiple machines to do the processing...but it's an idea that may be useful in another context so putting it down.

## Look ahead graph traversals

When a query is started, take advantage of the fact that we know all the vertices that will be visited.
For each vertex that is on a remote Node, issue a traversal request to the Node giving the algorithm to use and any traversal criteria. Where possible bulk these requests into one asyn action.

## Approaching as a classification problem

Data/Variables/features available include

* Number of nodes
* Number of vertices for each node
* Vertex density ratio for each node
* Number of vertices migrated to each node over a given time period

K-Means done per cluster???
Number of vertices per node must be balanced
&quot;Rigged Hash&quot; - Hashing a vertex ID goes to a node depending on clustering data

Two phased data organization.
Upon write, use consistent hashing to place vertex in the cluster.

Have a master node, &quot;Tesseri, i.e. Tesseract's eye&quot; whose sole job is to re-organize edges in the cluster to minimize network hops. 

Every edge is known to this node. With  available computing a 256GB machine is only $899 with hardware raided 1TB SSD and $679 with spindle disks from http://iweb.com/dedicated-server - there may be cheaper options.

This means practically speaking 20 billion vertices can be represented (8 bytes per vertex ID) in 150GB of RAM.

A data structure which borrows ideas from the log-structured merge tree means that even a much smaller RAM can be used, storing some data in memory and merging any overflow to an on-disk file/s. The obvious benefit to bigger ram is faster traversal in memory.

The Tesseract DB must have no single point of failure. As such the any node in the cluster can be elected to be the Tesseri. There are always at least two Nodes marked one is the current Tesseri and the other is the fallback should the first fail.

Only the current Tesseri has a copy of all edges in the cluster. If the current Tesseri fails it would have organized the cluster already so the backup Tesseri can aquire a copy of all edges, after a failure occurs. The assumption being that the backup Tesseri would be able aquire a copy of all edges before the graph becomes imbalanced enough to cause severe traversal problems.

While a node is the Tesseri it should be configurable that it is not considered a part of the cluster. So it doesn't take part in traversals and focus only on keeping the graph distribution balanced...alternatively it is worth exploring using the Tesseri as the node that performs all traversals...</content>
 </entry>
 
 <entry>
   <title>Query dissemination and result accumulation</title>
   <link href="http://research.zcourts.com/query-model.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}

## Dissemination

## Accumulation

## Query modes
Different cases may demand a different query mode depending on the amount of potential data in the result. There are two modes, namely immediate or synchronous and deferred or asynchronous modes.

### Immediate/Sync queries

Immediate query mode runs queries where the expected result set can easily fit in memory. When this query mode is used the query's leader disseminates the query, accumulates the result in its entirety before returning everything to the client.

### Deferred/Async queries

Deferred queries can handle a result set of any size. When the other nodes involved in the query return results the results are immediately pushed to the client.

## Persistent, long lived, real time graph traversals

In real-time situations queries can be left running. Only async queries can support this and as new vertices or edges are added that match a query, they are immediately accumulated and pushed to the client. If the client for a persistent query has disconnected, its data can be disk cached locally on the query's leader node. When the client reconnects the cached results can be delivered.

## Path prediction

Given a query and any data at any given time. Predict which vertex will be involved. If on a different node then issue the query on that node. And the results can all be accumulated. Does the resulting traversal converge, such that were the query done sequentially any vertex that would have been traversed has been?
If that is the case and the prediction can be made extremely early in the traversal then the result should be that any query is completely performed in parallel.
Perhaps one approach is to predict vertices every n-hops. Then using the resulting vertex to predict the next vertex. Where n is a configurable number &gt; 1.
The larger n is the more parallel a query is but the probability of an erronous prediction increases. Using this strategy a single query is dynamically broken down into multiple smaller queries accross the cluster.
[Path prediction algorithms](https://www.google.co.uk/search?q=path+prediction+algorithm&amp;oq=path+predi&amp;aqs=chrome.3.69i57j0l3.3517j0j1&amp;sourceid=chrome&amp;ie=UTF-8)

Maybe use some kind of marker technique to say that if you start from this vertex i you are guaranteed to get to this vertex j. Possibly precomputed marks? Or continuously computed marks so that when a query is run you're not running the prediction algorithm but using it's results from before. If it's continually re-computed as the graph changes then the &quot;predictions&quot; are guaranteed to be accurate at a given time (or at least until something is removed to cause it to become invalid).
When you hop from i to j then all vertices between are included in the result set.
When those vertices are processed if a query criteria isn't fulfilled then all the sub queries issued prior to the marker that failed the query can be terminated because their results will never be valid. This implies that the vertices from i to j cannot be returned in a result until the previous set of vertices have been verified to be valid for the query. 

This could result in unnecessary computations but significantly reduces the need to communicate a lot....maybe? that may still be affected by vetex placement in the cluster but if |j...i| is big enough the number of network hops required is already reduced anyway because none of the vertices between i and j will perform a network hop...unless the vertices between i and j create a branch which leads to another marker. but perhaps the markers from the prediction algorithm could be created such that none of the vertices between i and j will hit such a branch creating vertex.

With regards to extra computation, if |i...j| is small enough to be performed in micro or milliseconds the extra computation is likely to be cheaper than network hops since the entire operation is local.

### Super Fast Query Prediction

To take the path prediction to the next level. Real time queries are loaded and remain so on the server. As new data is added it is analyzed and marked. This means that when a request is made for the results of a query, the traversal can be initialized on all Nodes at the same time by using the pre-marked vertices as starting points.</content>
 </entry>
 
 <entry>
   <title>Reading List</title>
   <link href="http://research.zcourts.com/reading-list.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}

## Posts to be done

* Basically available soft state eventual consistency
* Bloom Filters, caching your way to performance
* CAP theorem
* Components of a database management system
* Designing a graph file system
* Gossip protocols
* Haskell all you need to know - Turned into [haskell.zcourts.com](http://haskell.zcourts.com)
* Haskell FFI, working with C libraries (see above)
* Imutability, a beautiful future
* Introduction to the fundamentals of graph theory
* Log structured merge trees for the rest of us (and why it doesn't work for graphs)
* Voltage Cluster (Finding communities in linear time: a physics approach)
* Markov cluster (http://micans.org/mcl/) Thesis =&gt; http://micans.org/mcl/index.html?sec_thesisetc
* Network modularity - http://en.wikipedia.org/wiki/Modularity_(networks)
* Clique - http://en.wikipedia.org/wiki/Clique_(graph_theory)
* Fuzy clustering - http://en.wikipedia.org/wiki/Fuzzy_clustering
* BIRCH - http://en.wikipedia.org/wiki/Birch_(data_clustering)

## TODO

Update with list of papers in the repo.


## Problems that can be solved with graphs

Having data represented as graphs is one thing but once you have that graph, what do you want to know about it? What can it tell you?
This is a collection of resources with ideas of how to use graphs or graph algorithms.

[Graphs from real life problems __question__](http://cstheory.stackexchange.com/questions/3409/graphs-from-real-life-problems)
[Data for testing graphs __question__](http://cstheory.stackexchange.com/questions/739/data-for-testing-graph-algorithms)
[Generating interesting combinatorial optimization problems](http://cstheory.stackexchange.com/questions/16751/generating-interesting-combinatorial-optimization-problems)

[Stanford network analysis project - SNAP](http://snap.stanford.edu/index.html)
[Data collection from SNAP](http://snap.stanford.edu/data/)

[DIMACS challenge data](http://www.dis.uniroma1.it/challenge9/download.shtml#benchmark)

[Stanford's GraphBase __Knuth__](http://www-cs-staff.stanford.edu/~knuth/sgb.html)

[Nauty and traces - new site](http://pallini.di.uniroma1.it/)
[Nauty and Traces - generate graphs, old site](http://cs.anu.edu.au/~bdm/nauty/)

[Clique Benchmark Instances](http://cs.hbg.psu.edu/benchmarks/clique.html)

[Graph colouring instances](http://mat.gsia.cmu.edu/COLOR/instances.html)

[The Open Graph Archive: A Community-Driven Effort](http://arxiv.org/abs/1109.1465)
[Graph archive](http://www.graph-archive.org/doku.php)

[UCI Network Data Repository](http://networkdata.ics.uci.edu/index.html)
[UCI Public dataset](http://networkdata.ics.uci.edu/resources.php)

[Washington state graph dataset](http://www.eecs.wsu.edu/mgd/gdb.html)

[GraphChi technique](http://graphlab.org/graphchi/)
[GraphLab's DataSet](http://graphlab.org/downloads/datasets/)

[Networks, Crowds and Markets, reasoning about a highly connected world](http://www.cs.cornell.edu/home/kleinber/networks-book/)

[Introduction to graph theory- Wikibooks](http://en.wikibooks.org/wiki/Graph_Theory/Introduction)

[Problem solving using graph traversals](http://www.slideshare.net/slidarko/problemsolving-using-graph-traversals-searching-scoring-ranking-and-recommendation)

[An Introduction to Graph Theory and Complex Networks](http://www.di.unipi.it/~ricci/book-watermarked.pdf)
</content>
 </entry>
 
 <entry>
   <title>Slinky</title>
   <link href="http://research.zcourts.com/red.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}

# Replicated Edge Distribution, RED

One of the big problems with distributed traversals is fetching all vertices involved in the query while minimizing the number of network hops. It follows that an ideal graph would have all the vertices required for a query on the same node. 

It is extremely difficult for an algorithm to always figure out the best way to partition a graph across multiple nodes, that would yield an ideal distribution for any query. Even if the user provided some kind of meta-data, a change in access pattern at the application level is likely to render the meta-data redundant or incorrect to the extent where the old distribution creates a bottle neck or causes too many network hops leading to network saturation or other failures.

On the premise that minimizing or removing network hops is an absolutely requirement which, if achieved will provide a huge improvement to query performance; It would be ideal if the entirety of any query could be performed on a single node. 

The problem with bringing all data on a cluster on to a single node is it defeats the purpose of having a cluster in the first place. However, observe that in order to perform a query only the edges are required.

Every edge that's created is sent to each replica node in the cluster.
The SVC stores the edge information (from and to vertices).

OR

Using consistent hashing a vertex goes to it's node and replicas
When an edge is added the edge is stored on both nodes the edge points to.
Starting a query from any node, consistent hashing determines which node becomes leader the same way the decision would be made of where to place the vertex.
When a query is run the leader will have a copy of all the edges needed to complete the query, without making any network requests.

For e.g. in a 3 node cluster with labels A,B,C, the following are created.
Vertices 1...10 
Edges (1,2),(2,3),(2,4),(1,4),(3,8),(4,8),(5,8),(4,6),(4,7),(2,6),(2,8),(1,9),(7,10) 

Assume vertex id ~= hash

Assume the data is distributed across the three nodes with consistent hashing such that the following distribution is achieved.

A 	B 	C
1	2	3
4	5	6
7	8	9
10	11	12

data Vertex = Vertex { name::String, id:Int }
data Edge = { from::Vertex, to::Vertex, directed::Bool }

add :: Vertex -&gt; None
add :: Edge -&gt; None


The following is a dry run of what would be added to each node A,B,C

Edge 	Send to 	Edges on A 				Edges on B 			Edges on C
===========================================================================
1,2		A,B 		1,2						1,2
2,3		B,C 								2,3					2,3				
2,4		A,B 		2,4						2,4
1,4		A 			1,4
3,8		B,C 								3,8					3,8
4,8		A,B 		4,8						4,8
5,8 	B 									5,8
4,6 	A,C 		4,6											4,6
4,7 	A 			4,7
2,6 	B,C 								2,6 				2,6
2,8 	B,C 								2,8					2,8
1,9 	A,C 		1,9 										1,9
7,10 	A 			7,10
===========================================================================
Total				8						8					6




The following are example queries:

Depth first search assuming a directed graph where no loops are allowed.

Starting from 4, which belongs to node A.
4-&gt;8-&gt;6-&gt;7-&gt;10

Starting from 1, which belongs to node A
1-&gt;2-&gt;4-&gt;8-&gt;6-&gt;7-&gt;10 __[FAIL]__ 2-&gt;3 but is not available on A.</content>
 </entry>
 
 <entry>
   <title>RSS Feed</title>
   <link href="http://research.zcourts.com/rss.xml"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;rss version=&quot;2.0&quot;&gt;
&lt;channel&gt;
        &lt;title&gt;{{ site.title }}&lt;/title&gt;
        &lt;description&gt;{{ site.title }} - {{ site.author.name }}&lt;/description&gt;
        &lt;link&gt;{{ site.production_url }}{{ site.rss_path }}&lt;/link&gt;
        &lt;link&gt;{{ site.production_url }}&lt;/link&gt;
        &lt;lastBuildDate&gt;{{ site.time | date_to_xmlschema }}&lt;/lastBuildDate&gt;
        &lt;pubDate&gt;{{ site.time | date_to_xmlschema }}&lt;/pubDate&gt;
        &lt;ttl&gt;1800&lt;/ttl&gt;

{% for post in site.posts %}
        &lt;item&gt;
                &lt;title&gt;{{ post.title }}&lt;/title&gt;
                &lt;description&gt;{{ post.content | xml_escape }}&lt;/description&gt;
                &lt;link&gt;{{ site.production_url }}{{ post.url }}&lt;/link&gt;
                &lt;guid&gt;{{ site.production_url }}{{ post.id }}&lt;/guid&gt;
                &lt;pubDate&gt;{{ post.date | date_to_xmlschema }}&lt;/pubDate&gt;
        &lt;/item&gt;
{% endfor %}


{% for page in site.pages %}
        &lt;item&gt;
                &lt;title&gt;{{ page.title }}&lt;/title&gt;
                &lt;description&gt;{{ page.content | xml_escape }}&lt;/description&gt;
                &lt;link&gt;{{ site.production_url }}{{ page.url }}&lt;/link&gt;
                &lt;guid&gt;{{ site.production_url }}{{ page.id }}&lt;/guid&gt;
                &lt;pubDate&gt;{{ page.date}}&lt;/pubDate&gt;
        &lt;/item&gt;
{% endfor %}

&lt;/channel&gt;
&lt;/rss&gt;
</content>
 </entry>
 
 <entry>
   <title>Sitemap</title>
   <link href="http://research.zcourts.com/sitemap.txt"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% for page in site.pages %}
{{site.production_url}}{{ page.url }}{% endfor %}
{% for post in site.posts %}
{{site.production_url}}{{ post.url }}{% endfor %}</content>
 </entry>
 
 <entry>
   <title>Tags</title>
   <link href="http://research.zcourts.com/tags.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}

&lt;ul class=&quot;tag_box inline&quot;&gt;
  {% assign tags_list = site.tags %}  
  {% include JB/tags_list %}
&lt;/ul&gt;


{% for tag in site.tags %} 
  &lt;h2 id=&quot;{{ tag[0] }}-ref&quot;&gt;{{ tag[0] }}&lt;/h2&gt;
  &lt;ul&gt;
    {% assign pages_list = tag[1] %}  
    {% include JB/pages_list %}
  &lt;/ul&gt;
{% endfor %}
</content>
 </entry>
 
 <entry>
   <title>Terminology</title>
   <link href="http://research.zcourts.com/terminology.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}

# Introduction

There are cases where terms overlap in definition, where context changes everything. I'll do my best to be consistent and follow the terms I set on this page, if in doubt a term is used with the intended definition being what it is on this page.

## Graph terms 

1. Vertex - A vertex is effectively a unit within a graph.
2. Edge - An edge connects to verticies and can be defined as e = (A1,B1)
3. A graph G is made up of a set of verticies and edges, G = (V,E)
4. An undirected graph is one where no distinction is made between an edge's two verticies.
5. A directed graph is one where each edge determines it's direction, i.e. e= (A1,B1) is interpretted as A1 points to B1.
6. A mixed graph is on which contains both directed and undirected edges.
7. The order of a graph is the number of verticies |V|
8. The size of a graph is the number of edges |E|
9. The degree of a vertex is the number of edges that connect to it
10. A loop is an edge whose two vertices are the same i.e. loop  e = (A1,A1), a loop is counted twice when determining a vertex's degree.
11. In-degree of a vertex is the number of edges connecting to it in a directed graph
12. Out-degree of a vertex is the number of edges leaving a vertex in a directed graph.
13. Two edges are said to be adjacent if they share a common vertex
14. Two vertices are said to be adjacent if they are connected by an edge, i.e. the edge e = (a,b) makes the vertices a and b adjacent.

## Other terms

1. Node - A node is</content>
 </entry>
 
 <entry>
   <title>Tesseract Graph Query language, TGQ</title>
   <link href="http://research.zcourts.com/tgq.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}

# Abstract - Introduction

The Tesseract Graph Query Language (TGQ) is inspired by SQL{% cite coddsql %}, not in syntax but by purpose. SQL allows querying, creating, updating and deleting (CRUD) of data in a relational database {% cite coddrelational %}. TGQ serves the same purpose. While it is written for The Tesseract's [query model](/query-model.html) in particular, it is general enough to apply to other graph storage systems.

## References

{% bibliography -c %}</content>
 </entry>
 
 <entry>
   <title>Vertex Drift</title>
   <link href="http://research.zcourts.com/vertex-drift.html"/>
   <updated></updated>
   <id>http://research.zcourts.com</id>
   <content type="html">{% include JB/setup %}

# Abstract - Introduction

Here I propose &quot;Vertex Drift&quot; a dynamic partitioning algorithm suited to the [query model](/query-model.html) being used in the Tesseract. Most current dynamic graph partition algorithms are focused on inter processor communication and data migration, i.e. on a single node. {% cite walshaw1997parallel %} for example introduces the relative gain optimization technique. This and other algorithms require some amount of data migration or are iterative techniques. Vertex drift requires neither iteration or data migration and instead of working with multiple processors it is focused on working with multiple machines. It borrows ideas from Dynamo's virtual nodes {% cite dynamo %} to create a technique where by a vertex or some of it's data splits, and pieces &quot;drift&quot; around the cluster. The Tesseract query model where &quot;markers&quot; and path prediction are used, means that when a vertex is drifted around the cluster we know a lot about a vertex in advance. Using the vertex drift algorithm, only {%m%}O(2n){%em%} messages are required for a traversal (in the worse case) which involves the vertex, where  {%m%}n{%em%} is the number of nodes in the cluster. Only  {%m%}O(n){%em%} messages are required in the best case.

# Background - Why is this needed?

Every day graphs as they occur in the real world can be very disproportionate. This makes it tricky to create an algorithm that will automatically partition data in a reasonable way that ensures minimum network communication during traversals.

As an example, consider the social graph for some Twitter users with 40+ million followers, while the average number of followers for a twitter user is only about 200. The following images represent the number of followers, those they follow and Tweets from Justin Bieber, Kety Perry, Lady Gaga and myself.

![Number of Justin Bieber followers](/assets/bieber-graph.png &quot;Number of Justin Bieber followers&quot;)
![Number of Katy Perry followers](/assets/perry-graph.png &quot;Number of Kety Perry followers&quot;)
![Number of Lady Gaga followers](/assets/gaga-graph.png &quot;Number of Lady Gaga followers&quot;)
![Number of Courtney Robinson's followers](/assets/zcourts-graph.png &quot;Number of Courtney Robinson's followers&quot;)

These images form a clear demonstration of how ill proportioned a real world graph can be. The question now becomes, how can the relationship of each of these disproportionate vertices be stored such that, traversal is evenly spread across the cluster. One answer is vertex drift, by splitting up the number of in and out edges across multiple machines with this technique a traversal involving this vertex can run against in or outbound edges in parallel across the entire cluster.

# Edge partitioning

Based on the concepts in [virtual nodes](http://www.datastax.com/dev/blog/virtual-nodes-in-cassandra-1-2) from Dynamo and implemented in Cassandra. While the application is different it is a similar concept with optimizations for graphs.


## References

{% bibliography -c %}</content>
 </entry>
 
 
</feed>